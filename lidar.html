<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LiDAR Setup & Integration</title>
  <link rel="stylesheet" href="css/style.css" />
</head>
<body>
  <header>
    <h1>LiDAR System Guide (Raspberry Pi + ROS 2 Humble)</h1>
    <nav>
      <a href="index.html">Home</a>
      <a href="parts.html">Parts List</a>
      <a href="sensor.html">Sensor Setup</a>
      <a href="lidar.html">LiDAR Car</a>
    </nav>
    <button onclick="toggleDarkMode()">üåô Toggle Dark Mode</button>
  </header>

      <section class="card">
      <details>
        <summary>OS Setup</summary>
    
        <h3>Requirements</h3>
        <ul>
          <li>Raspberry Pi 4B (or 3B+)</li>
          <li>MicroSD Card (32GB or more, Class 10 recommended)</li>
          <li>MicroSD Card Reader</li>
          <li>Power Supply (5V 3A)</li>
          <li>HDMI Monitor, USB Keyboard and Mouse (for first-time setup)</li>
        </ul>
    
        <details>
  <summary>Step 1: Installing Ubuntu</summary>

  <p>This guide covers the complete setup process for your Raspberry Pi 4B (8GB), including Ubuntu installation, GUI, and VNC configuration.</p>

  <h4>1. Ubuntu 22.04.2 LTS Installation</h4>
  <p>Works with ROS2</p>

  <p>Download Raspberry Pi Imager from:</p>
  <pre><code>https://www.raspberrypi.com/software/</code></pre>

  <p>Use it to flash the image to a 32GB+ microSD card:</p>
  <pre><code>Choose OS ‚Üí Other general-purpose OS ‚Üí Ubuntu ‚Üí Ubuntu Server 22.04.2 LTS (64-bit)</code></pre>

  <p>Choose your storage location and click <strong>Write</strong>.</p>
  <p>Optionally configure Wi-Fi by editing:</p>
  <pre><code>network-config</code></pre>

  <p>Insert the microSD into your Pi, power it on, and make sure it‚Äôs connected to a monitor to boot properly.</p>

  <h4>2. System Update and GUI Installation</h4>
  <p>GUI = graphical user interface</p>

  <p>Run system update:</p>
  <pre><code>sudo apt update && sudo apt upgrade -y</code></pre>

  <p>If prompted to update the kernel, choose 'yes' and keep default selections.</p>

  <p>Install XFCE and LightDM:</p>
  <pre><code>sudo apt install xfce4 lightdm -y</code></pre>

  <p>Reboot:</p>
  <pre><code>sudo reboot</code></pre>

</details>

        
  <details>
  <summary>Step 2: ROS 2 Setup</summary>

  <h4>5. Install ROS 2 Humble</h4>
  <p>ROS 2 Humble was chosen because it's a dedicated operating system framework designed for robotic control and communication.</p>

  <p>Update the system and configure locale:</p>
  <pre><code>sudo apt update && sudo apt upgrade -y
sudo apt install locales -y
sudo locale-gen en_US en_US.UTF-8
sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8</code></pre>

  <p>Add ROS 2 sources and install:</p>
  <pre><code>sudo apt install curl gnupg lsb-release -y
sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg</code></pre>

  <p>Add source list:</p>
  <pre><code>echo "deb [signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null</code></pre>

  <p>Update again and install ROS 2 Humble Desktop:</p>
  <pre><code>sudo apt update
sudo apt install ros-humble-desktop -y</code></pre>

  <p>Source ROS in your shell:</p>
  <pre><code>echo "source /opt/ros/humble/setup.bash" >> ~/.bashrc
source ~/.bashrc</code></pre>

  <p>Test installation:</p>
  <pre><code>ros2 run demo_nodes_cpp talker</code></pre>

  <p><em>(If the installation went correctly, you should see ROS output messages in the terminal.)</em></p>
</details>



<details>
  <summary>Step 3: Creating ROS2 Environment</summary>

  <p><strong>Create ROS 2 Workspace</strong></p>
  <p>ROS 2 workspace serves as a dedicated directory for organizing and developing your robot‚Äôs software.</p>

  <p>Install colcon:</p>
  <pre><code>sudo apt install python3-colcon-common-extensions -y</code></pre>
  <!-- [Insert Screenshot] -->

  <p>Create and build workspace:</p>
  <pre><code>mkdir -p ~/ros2_ws/src
cd ~/ros2_ws
colcon build
source install/setup.bash</code></pre>

</details>


          <details>
              <summary>Step 4: Remote Access & Development Setup</summary>
              <details>
                <summary>Step 1: Download and Install VS Code</summary>
                <ol>
                  <li>Go to <a href="https://code.visualstudio.com/" target="_blank">Visual Studio Code</a> and download the version for your OS (Windows, macOS, or Linux).</li>
                  <li>Install it using the standard installer.</li>
                  <li>Launch VS Code and open the Extensions tab (left sidebar).</li>
                  <li>Search for and install the extension called <strong>"Remote - SSH"</strong>.</li>
                </ol>
              </details>
              <details>
                <summary>Step 2: Set Up Remote Access Options</summary>
                <h4>Option A: Tailscale</h4>
                <p><strong>Tailscale</strong> is a secure mesh VPN that connects your devices together into a private network using WireGuard.</p>
          
                <h5>On Your Computer:</h5>
                <ol>
                  <li>Visit <a href="https://tailscale.com/download" target="_blank">tailscale.com/download</a> and install Tailscale.</li>
                  <li>Sign in using GitHub, Google, or Microsoft account.</li>
                  <li>Click <strong>"Enable Tailscale"</strong> to activate the virtual network.</li>
                </ol>
          
                <h5>On Your Raspberry Pi:</h5>
                <div class="code-block">
                  <pre><code>curl -fsSL https://tailscale.com/install.sh | sh
          sudo tailscale up</code></pre>
                </div>
                <p>Log in via browser when prompted. The Pi will appear in your Tailscale dashboard with a private IP (e.g., <code>100.101.102.103</code>).</p>
          
                <h4>Option B: Setup VNC</h4>
                <p><strong>VNC</strong> (Virtual Network Computing) is a remote desktop-sharing system that lets you control your Raspberry Pi's GUI from another device over a network.</p>
                <ol>
                  <li>Install VNC:
                    <pre><code>sudo apt install x11vnc -y</code></pre>
                   <!-- [Insert Screenshot] -->
                  </li>

                  <li>Optional: Set up auto-login:
                    <pre><code>sudo nano /etc/lightdm/lightdm.conf</code></pre>
                   Paste the following (replace <code>ubuntu</code> with your username):
                   <pre><code>[Seat:*]
                autologin-user=ubuntu
                autologin-user-timeout=0
                user-session=ubuntu
                greeter-session=lightdm-gtk-greeter</code></pre>
                    Save with <code>Ctrl+O</code>, <code>Enter</code>, then <code>Ctrl+X</code>.
                   <!-- [Insert Screenshot] -->
                 </li>

                  <li>Start the VNC server:
                   <pre><code>x11vnc -usepw -forever -display :0</code></pre>
                   <!-- [Insert Screenshot] -->
                  </li>

                  <li>Connect from another device on the same Wi-Fi using the Pi‚Äôs IP address.
                    <br>You can get it with:
                    <pre><code>hostname -I</code></pre>
                   You may need to add <code>:5900</code> or <code>:5901</code> to the IP.
                   <br>If using a mobile hotspot, check the hotspot device list to find the Pi‚Äôs IP.
                 </li>
                </ol>
              </details>
          
              <details>
                <summary>Step 3: Connect via VS Code (SSH)</summary>
                <ol>
                  <li>In VS Code, press <kbd>Cmd/Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd>.</li>
                  <li>Select <strong>Remote-SSH: Connect to Host</strong>.</li>
                  <li>Enter the Pi's Tailscale IP like:
                    <div class="code-block"><code>pi@100.101.102.103</code></div>
                  </li>
                  <li>Authenticate with your Pi's password.</li>
                  <li>You're now working directly on the Pi through VS Code with full file system access.</li>
                </ol>
          
                <h3>ü§∫ Which Should You Use?</h3>
                <div class="tip-box">
                  <strong>üîß Use VS Code with Tailscale if you:</strong>
                  <ul>
                    <li>Need to write or edit code (Python, HTML, etc.)</li>
                    <li>Want terminal + Git + live development tools</li>
                    <li>Prefer using an IDE over a browser</li>
                  </ul>
                </div>
                <div class="tip-box">
                  <strong>üåê Use VNC if you:</strong>
                  <ul>
                    <li>You need full access to the Raspberry Pi desktop environment remotely.</li>
                    <li>You want to interact with GUI apps as if sitting at the Pi.</li>
                    <li>You're on the same local Wi-Fi network or know your Pi‚Äôs IP address.</li>
                    <li>You prefer an open-source, customizable remote desktop solution.</li>

                  </ul>
                </div>
                <p><strong>Pro Tip:</strong> You can use both! Use VNC for quick GUI tweaks, and Tailscale+SSH for serious development work.</p>
              </details>
            </section>


    <section class="card">
      <details>
        <summary>USB Camera & Lidar Setup</summary>
    
        <h3>What You Need</h3>
        <ul>
          <li>Raspberry Pi 4B (or 3B+)</li>
          <li>Flashed MicroSD card with Ubuntu and Ros2</li>
          <li>Power Supply (5V 3A)</li>
          <li>RP LiDAR C1</li>
          <li>HDMI Monitor, USB Keyboard and Mouse OR SSH & VNC</li>
        </ul>
<details>
  <summary>üñ•Ô∏è Setup RP Lidar C1</summary>

  <p><strong>What is LiDAR?</strong></p>
  <p>
    LiDAR stands for Light Detection and Ranging. It‚Äôs a special kind of sensor that uses laser beams to measure how far things are around it.
    You can think of it like a superhero‚Äôs radar‚Äîit spins and shoots out tiny lasers, then checks how long it takes for them to bounce back.
    That‚Äôs how it knows where walls, chairs, or even people are! Robots use LiDAR to ‚Äúsee‚Äù and move around safely without bumping into things.
  </p>

  <p><strong>How to Set Up RPLIDAR C1 on ROS 2 (Ubuntu)</strong><br>
  Let‚Äôs install the LiDAR on a Raspberry Pi running Ubuntu and ROS 2 Humble. Follow these simple steps:</p>

  <p><strong>Step 1: Set up your ROS 2 environment</strong></p>
  <pre><code>source /opt/ros/humble/setup.bash</code></pre>

  <p><strong>Step 2: Create a new workspace folder</strong><br>This is where you‚Äôll store your LiDAR files:</p>
  <pre><code>cd ~
mkdir -p ~/ws_lidar/src
cd ~/ws_lidar/src</code></pre>

  <p><strong>Step 3: Download the LiDAR software</strong></p>
  <pre><code>git clone https://github.com/Slamtec/sllidar_ros2.git</code></pre>

  <p><strong>Step 4: Check if the folder was downloaded</strong></p>
  <pre><code>ls -la</code></pre>
  <p>You should see a folder named <code>sllidar_ros2</code>.</p>

  <p><strong>Step 5: Build the workspace</strong></p>
  <pre><code>cd ~/ws_lidar/
colcon build --symlink-install</code></pre>
  <p>If you see warnings (stderr), don‚Äôt worry! They're just messages‚Äînot errors.</p>

  <p><strong>Step 6: Check if the build worked</strong></p>
  <pre><code>ls -la</code></pre>
  <p>Look for: <code>build</code>, <code>install</code>, <code>log</code>, and <code>src</code></p>

  <p><strong>Step 7: Tell your system about the new software</strong></p>
  <pre><code>source ~/ws_lidar/install/setup.bash</code></pre>

  <p><strong>Step 8: Plug in your RPLIDAR via USB</strong></p>
  <pre><code>ls -la /dev | grep USB</code></pre>
  <p>Look for something like:</p>
  <pre><code>crw-rw---- 1 root dialout ... ttyUSB0</code></pre>

  <p><strong>Step 9: Give permission to access the LiDAR</strong></p>
  <pre><code>sudo chmod 777 /dev/ttyUSB0</code></pre>
  <p>Double check:</p>
  <pre><code>ls -la /dev | grep USB</code></pre>
  <p>Now it should say:</p>
  <pre><code>crwxrwxrwx ...</code></pre>

  <p><strong>Step 10: Launch the LiDAR!</strong></p>
  <pre><code>ros2 launch sllidar_ros2 view_sllidar_c1_launch.py</code></pre>

  <p><strong>Step 11: Open RViz2 to See the LiDAR Scan</strong></p>
  <pre><code>rviz2</code></pre>
  <p>RViz2 is like the robot‚Äôs eyes! It shows a live 2D scan of what the LiDAR "sees" around it.</p>

  <p><strong>Step 12: Set the RViz2 Display Settings</strong><br>Once RViz2 opens, follow these steps:</p>
  <ol>
    <li>On the left panel, click <code>Add</code> ‚Üí choose <code>LaserScan</code>.</li>
    <li>Set the LaserScan Topic to: <code>/scan</code></li>
    <li>Set the Fixed Frame (top left dropdown) to: <code>laser</code></li>
  </ol>
  <p>Now you should start seeing dots or lines representing objects around the LiDAR!</p>

  <p><strong>Step 13: What if RViz2 is not installed?</strong><br>If your terminal says:</p>
  <pre><code>Command 'rviz2' not found</code></pre>
  <p>Then install it using this command:</p>
  <pre><code>sudo apt update
sudo apt install ros-humble-rviz2</code></pre>
  <p>Once installed, try again:</p>
  <pre><code>rviz2</code></pre>
</details>

        
        <details>
  <summary>Logitech Brio 100 USB Camera Setup</summary>

  <p><strong>How to Set Up a USB Camera on ROS 2 (Logitech Brio 100)</strong><br>
  This guide will help you install and run a USB camera (like the Logitech Brio 100) using ROS 2 and Ubuntu. We‚Äôll use two nodes: one to publish raw camera frames (<code>/image_raw</code>) and another to publish compressed video (<code>/image_raw/compressed</code>).</p>

  <p><strong>Step 1: Plug in the USB Camera</strong><br>
  Connect your USB camera (e.g., Logitech Brio 100) to the Raspberry Pi or your PC using a USB port.<br>
  You can check if it‚Äôs connected by running:</p>
  <pre><code>ls /dev/video*</code></pre>
  <p>If you see <code>/dev/video0</code>, the camera is detected.</p>

  <p><strong>Step 2: Install ROS 2 Camera Packages</strong><br>
  Make sure these packages are installed:</p>
  <pre><code>sudo apt update
sudo apt install ros-humble-image-tools ros-humble-v4l2-camera</code></pre>

  <p><strong>Step 3: Launch the Raw Image Publisher</strong><br>
  Use the <code>v4l2_camera</code> node to start publishing camera frames to <code>/image_raw</code>:</p>
  <pre><code>ros2 run v4l2_camera v4l2_camera_node</code></pre>
  <p>By default, it will start broadcasting frames to <code>/image_raw</code>.</p>

  <p><strong>Step 4: Check the Topic</strong><br>
  To verify if the camera is streaming, run:</p>
  <pre><code>ros2 topic list</code></pre>
  <p>You should see <code>/image_raw</code> in the list.</p>

  <p><strong>Step 5: Launch the Compressed Image Publisher</strong><br>
  Use this command to publish compressed video to <code>/image_raw/compressed</code>:</p>
  <pre><code>ros2 run image_transport republish raw in:=/image_raw compressed:=/image_raw/compressed</code></pre>

  <p><strong>Step 6: Verify Output</strong><br>
  To check if both raw and compressed image streams are working, use:</p>
  <pre><code>ros2 topic echo /image_raw
ros2 topic echo /image_raw/compressed</code></pre>

  <p><strong>Step 7: View the Camera Feed</strong><br>
  You can visualize the live camera feed in RViz2 by adding an <code>Image</code> display:</p>
  <ol>
    <li>Open RViz2:
      <pre><code>rviz2</code></pre>
    </li>
    <li>Click <code>Add</code> and choose <code>Image</code>.</li>
    <li>Set the topic to <code>/image_raw</code> or <code>/image_raw/compressed</code>.</li>
    <li>Set the fixed frame to <code>camera_link</code> (or whichever TF frame is being used).</li>
  </ol>

  <p><strong>Step 8: Fixing Missing RViz2 or Errors</strong></p>
  <p>If <code>rviz2</code> isn‚Äôt installed, install it using:</p>
  <pre><code>sudo apt install ros-humble-rviz2</code></pre>

  <p>If you get device or permission errors, make sure <code>/dev/video0</code> has the right permissions:</p>
  <pre><code>sudo chmod 777 /dev/video0</code></pre>

  <p><strong>Step 9: Common Errors and Fixes</strong><br>Here are some common errors you might run into and how to fix them:</p>
  <ul>
    <li><b>Error:</b> <code>rviz2: command not found</code><br>
        <b>Fix:</b>
      <pre><code>sudo apt install ros-humble-rviz2</code></pre>
    </li>

    <li><b>Error:</b> <code>Cannot open '/dev/video0'</code> or <code>Permission denied</code><br>
        <b>Fix:</b>
      <pre><code>sudo chmod 777 /dev/video0</code></pre>
    </li>

    <li><b>Error:</b> <code>No image received on topic /image_raw</code><br>
        <b>Fixes:</b>
      <ul>
        <li>Make sure the camera is properly connected.</li>
        <li>Ensure the camera node is running:<br><code>ros2 run v4l2_camera v4l2_camera_node</code></li>
        <li>Check if <code>/image_raw</code> topic exists:<br><code>ros2 topic list</code></li>
        <li>Use:<br><code>ros2 topic echo /image_raw</code> to see live data.</li>
      </ul>
    </li>
  </ul>

  <p><strong>Step 10: View Camera Feed in RViz2</strong><br>To visualize the live video from your USB camera in RViz2:</p>
  <ol>
    <li>Launch RViz2:
      <pre><code>rviz2</code></pre>
    </li>
    <li>In RViz2:
      <ul>
        <li>Click the <code>Add</code> button in the bottom left.</li>
        <li>Choose <code>Image</code> from the list.</li>
      </ul>
    </li>
    <li>In the Image settings:
      <ul>
        <li>Set the topic to:<br><code>/image_raw/compressed</code></li>
      </ul>
    </li>
  </ol>

  <p>Now you should see your camera stream inside RViz2!</p>
</details>
            </details>
          </section>


        <section class="card">
      <details>
        <summary>Control Car Using PS4 Controller</summary>
    
<h3>Information on the Motor Hat and information on a Power Source</h3>

<ul>
  <li>The Cokoino 4WD Motor HAT uses two DRV8833 motor drivers to control four DC motors.</li>
  <li>It accepts a 7V‚Äì12.6V input on VIN (we used 18650 9900mAh batteries 3.7V actual ~4V each).</li>
  <li>It steps down voltage to 5V to safely power the Raspberry Pi via GPIO header.</li>
  <li>Ensure the battery supplies enough current for the Pi, camera, LiDAR, and all motors.</li>
  <li><strong>Power and GPIO Overview:</strong>
    <ul>
      <li>VIN: 7V‚Äì12.6V input</li>
      <li>5V: Output to Pi</li>
      <li>IN1‚ÄìIN4 and ENA/ENB: Control GPIO</li>
      <li>PWM: Controlled via RPi.GPIO</li>
    </ul>
  </li>
  <li><strong>Motor wiring:</strong>
    <ul>
      <li>Motor 1 = Front Left</li>
      <li>Motor 2 = Front Right</li>
      <li>Motor 3 = Rear Left</li>
      <li>Motor 4 = Rear Right</li>
    </ul>
  </li>
  <li>Each driver uses two GPIOs per motor for direction, and PWM for speed control.</li>
</ul>

    
<details>
  <summary>üñ•Ô∏è Configure Mecanum Drive Node with PS4 Controller</summary>
<p><strong>This is where you create a NODE that is dedicated to the software that will be controlling the movement of your wheels </strong><br>
  
  <p><strong>Create the package:</strong></p>
  <pre><code>ros2 pkg create --build-type ament_python ps4_mecanum --dependencies rclpy sensor_msgs</code></pre>

  <p><strong>Add <code>mecanum_drive_node.py</code> with your motor control logic:</strong></p>
  <pre><code>gedit ~/ros2_ws/src/ps4_mecanum/ps4_mecanum/mecanum_drive_node.py</code></pre>

  <pre><code>import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Joy
import RPi.GPIO as GPIO

PWM_FREQ = 1000  # Hz

# GPIO pins for each motor (adjust as needed)
MOTOR_PINS = {
    "FL": {"fwd": 17, "bwd": 18},
    "FR": {"fwd": 22, "bwd": 23},
    "RL": {"fwd": 24, "bwd": 25},
    "RR": {"fwd": 5,  "bwd": 6},
}

class MecanumDrive(Node):
    def __init__(self):
        super().__init__('mecanum_drive_node')
        self.subscription = self.create_subscription(Joy, 'joy', self.joy_callback, 10)
        GPIO.setmode(GPIO.BCM)
        self.pwm = {}

        for label, pins in MOTOR_PINS.items():
            GPIO.setup(pins["fwd"], GPIO.OUT)
            GPIO.setup(pins["bwd"], GPIO.OUT)
            self.pwm[label] = {
                "fwd": GPIO.PWM(pins["fwd"], PWM_FREQ),
                "bwd": GPIO.PWM(pins["bwd"], PWM_FREQ)
            }
            self.pwm[label]["fwd"].start(0)
            self.pwm[label]["bwd"].start(0)

        self.get_logger().info("? Mecanum Drive Node Initialized")

    def set_motor(self, label, value):
        value = max(min(value, 100), -100)
        if value > 0:
            self.pwm[label]["fwd"].ChangeDutyCycle(value)
            self.pwm[label]["bwd"].ChangeDutyCycle(0)
        elif value < 0:
            self.pwm[label]["fwd"].ChangeDutyCycle(0)
            self.pwm[label]["bwd"].ChangeDutyCycle(-value)
        else:
            self.pwm[label]["fwd"].ChangeDutyCycle(0)
            self.pwm[label]["bwd"].ChangeDutyCycle(0)

    def joy_callback(self, msg):
        try:
            x = float(msg.axes[0])
            y = -float(msg.axes[1])
            rot = float(msg.axes[2])

            fl = y + x + rot
            fr = y - x - rot
            rl = y - x + rot
            rr = y + x - rot

            max_val = max(abs(fl), abs(fr), abs(rl), abs(rr), 1)
            fl = int((fl / max_val) * 100)
            fr = int((fr / max_val) * 100)
            rl = int((rl / max_val) * 100)
            rr = int((rr / max_val) * 100)

            self.set_motor("FL", fl)
            self.set_motor("FR", fr)
            self.set_motor("RL", rl)
            self.set_motor("RR", rr)

        except Exception as e:
            self.get_logger().error(f"Joy callback failed: {e}")

    def destroy_node(self):
        for motor in self.pwm.values():
            motor["fwd"].stop()
            motor["bwd"].stop()
        GPIO.cleanup()
        super().destroy_node()

def main(args=None):
    rclpy.init(args=args)
    node = MecanumDrive()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>

  <p><strong>Edit <code>setup.py</code> to include correct entry points:</strong></p>
  <pre><code>touch ~/ros2_ws/src/ps4_mecanum/ps4_mecanum/__init__.py</code></pre>

  <p><strong>Navigate to the package folder and create the node file:</strong></p>
  <pre><code>cd ~/ros2_ws/src/ps4_mecanum/ps4_mecanum
gedit mecanum_drive_node.py</code></pre>

  <p><strong>Update <code>setup.py</code>:</strong></p>
  <pre><code>from setuptools import find_packages, setup

package_name = 'ps4_mecanum'

setup(
    name=package_name,
    version='0.0.0',
    packages=find_packages(exclude=['test']),
    data_files=[
        ('share/ament_index/resource_index/packages',
            ['resource/' + package_name]),
        ('share/' + package_name, ['package.xml']),
    ],
    install_requires=['setuptools'],
    zip_safe=True,
    maintainer='hugo',
    maintainer_email='hugo@todo.todo',
    description='TODO: Package description',
    license='TODO: License declaration',
    tests_require=['pytest'],
    entry_points={
        'console_scripts': [
            'mecanum_drive_node = ps4_mecanum.mecanum_drive_node:main',
        ],
    },
)</code></pre>

  <p><strong>Build the package:</strong></p>
  <pre><code>cd ~/ros2_ws
colcon build
source install/setup.bash</code></pre>

  <p><strong>Run <code>joy</code> and mecanum nodes in two separate terminals:</strong></p>

  <p>Terminal 1:</p>
  <pre><code>source ~/ros2_ws/install/setup.bash
ros2 run joy joy_node</code></pre>

  <p>Terminal 2:</p>
  <pre><code>source ~/ros2_ws/install/setup.bash
ros2 run ps4_drive ps4_drive_node</code></pre>

  <p>If you see 'RPi' import errors:</p>
  <pre><code>sudo apt install python3-rpi.gpio -y</code></pre>
</details>


        
<details>
  <summary>Connect and Pair PS4 Controller</summary>

  <p>ROS 2 was used because it is a dedicated operating system framework designed for robot operation.</p>

  <p><strong>Reset Bluetooth:</strong></p>
  <pre><code>sudo rfkill unblock bluetooth
sudo systemctl restart bluetooth
bluetoothctl</code></pre>

  <p><strong>Inside <code>bluetoothctl</code>, type the following commands one by one:</strong></p>
  <pre><code>power on
agent on
default-agent
scan on</code></pre>

  <p>üéÆ Hold <strong>PS + Share</strong> on the controller until it blinks rapidly.<br>
  Then pair, trust, and connect (replace &lt;MAC&gt; with the controller's address):</p>
  <pre><code>pair &lt;MAC&gt;
trust &lt;MAC&gt;
connect &lt;MAC&gt;</code></pre>

  <p><strong>Confirm controller is connected:</strong></p>
  <pre><code>ls /dev/input/</code></pre>
  <p>You should see <code>js0</code>.</p>

  <pre><code>jstest /dev/input/js0</code></pre>
</details>
            </details>
          </section>


<section class="card">
  <details>
    <summary>Obstacle Avoidance with LiDAR and Mecanum Wheels</summary>

              <!-- Video 1: Autonomous Navigation -->
          <div class="video-block">
            <h3>Obstracle Avoidance</h3>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/TEothjIMlyU"
              title="Obstracle Avoidance" frameborder="0" allowfullscreen></iframe>
          </div>

    <h4>Goal</h4>
    <p>Build a robot that:</p>
    <ul>
      <li>Detects obstacles using an RPLiDAR A1 sensor.</li>
      <li>Knows the direction of the obstacle (front-left, front-right, back, left, right).</li>
      <li>Decides how to avoid them by moving forward, turning, or crab-walking using mecanum wheels.</li>
    </ul>

    <h4>What You Need</h4>
    <ul>
      <li>Raspberry Pi 4 with Ubuntu 22.04 64-bit</li>
      <li>ROS 2 Humble installed</li>
      <li>RPLiDAR A1 connected and publishing <code>/scan</code> topic</li>
      <li>Adafruit Motor HAT (controls 4 DC motors)</li>
      <li>Mecanum wheel chassis with 4 motors wired to Motor HAT</li>
      <li>Python libraries: <code>adafruit-circuitpython-motorkit</code>, <code>rclpy</code>, <code>sensor_msgs</code></li>
    </ul>

    <h4>Step 1: Create a New ROS 2 Package</h4>
    <pre><code>cd ~/ws_lidar/src
ros2 pkg create --build-type ament_python obstacle_avoidance</code></pre>

    <h4>Step 2: Create the Control Script</h4>
    <pre><code>cd ~/ws_lidar/src/obstacle_avoidance/obstacle_avoidance
touch obstacle_avoidance_controller.py
chmod +x obstacle_avoidance_controller.py
nano obstacle_avoidance_controller.py</code></pre>

<p>Paste the following code into <code>obstacle_avoidance_controller.py</code>:</p>
<pre><code>import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan
from adafruit_motorkit import MotorKit
import termios, tty, sys, select, time, math

class ObstacleAvoidanceNode(Node):
    def __init__(self):
        super().__init__('obstacle_avoidance_controller')

        self.kit = MotorKit()
        self.motor_fl = self.kit.motor2
        self.motor_fr = self.kit.motor3
        self.motor_bl = self.kit.motor4
        self.motor_br = self.kit.motor1

        self.thrust = 0.5
        self.active = True

        self.create_subscription(LaserScan, '/scan', self.lidar_callback, 10)
        self.get_logger().info('Obstacle avoidance started')

        self.timer = self.create_timer(0.1, self.keyboard_check)

    def get_distance(self, msg, angle_deg):
        angle_rad = math.radians(angle_deg)
        index = int((angle_rad - msg.angle_min) / msg.angle_increment)
        if 0 <= index < len(msg.ranges):
            return msg.ranges[index]
        return float('inf')

    def lidar_callback(self, msg):
        if not self.active:
            self.stop_all()
            return

        front_right_angles = [-170, -160, -150, -140]
        front_left_angles = [170, 160, 150, 140, 180]
        back_angles = [0, 10, 20, 30, 40, -10, -20, -30, -40]
        left_angles = [90, 75, 105]
        right_angles = [-90, -75, -105]

        for angle in front_right_angles:
            dist = self.get_distance(msg, angle)
            self.get_logger().info(f'Front-Right {angle}¬∞: {dist:.2f} m')
            if dist < 0.5:
                self.get_logger().info(f'Obstacle on Front-Right! Reverse + Turn LEFT...')
                self.move(-1.0, 0.0, 0.0, duration=0.5)
                self.move(0.0, 0.0, -1.0, duration=0.6)
                self.move(1.0, 0.0, 0.0)
                return

        for angle in front_left_angles:
            dist = self.get_distance(msg, angle)
            self.get_logger().info(f'Front-Left {angle}¬∞: {dist:.2f} m')
            if dist < 0.5:
                self.get_logger().info(f'Obstacle on Front-Left! Reverse + Turn RIGHT...')
                self.move(-1.0, 0.0, 0.0, duration=0.5)
                self.move(0.0, 0.0, 1.0, duration=0.6)
                self.move(1.0, 0.0, 0.0)
                return

        for angle in back_angles:
            dist = self.get_distance(msg, angle)
            self.get_logger().info(f'Back {angle}¬∞: {dist:.2f} m')
            if dist < 0.15:
                self.get_logger().info(f'Obstacle in back! Forward + Turn RIGHT...')
                self.move(1.0, 0.0, 0.0, duration=0.5)
                self.move(0.0, 0.0, 1.0, duration=0.6)
                return

        for angle in left_angles:
            dist = self.get_distance(msg, angle)
            self.get_logger().info(f'Left {angle}¬∞: {dist:.2f} m')
            if dist < 0.12:
                self.get_logger().info(f'Obstacle on LEFT! Crabwalk RIGHT...')
                self.move(0.0, 1.0, 0.0, duration=1.3)
                self.move(1.0, 0.0, 0.0)
                return

        for angle in right_angles:
            dist = self.get_distance(msg, angle)
            self.get_logger().info(f'Right {angle}¬∞: {dist:.2f} m')
            if dist < 0.12:
                self.get_logger().info(f'Obstacle on RIGHT! Crabwalk LEFT...')
                self.move(0.0, -1.0, 0.0, duration=1.3)
                self.move(1.0, 0.0, 0.0)
                return

        self.move(1.0, 0.0, 0.0)

    def move(self, forward, strafe, rotate, duration=None):
        fl = forward + strafe + rotate
        fr = forward - strafe - rotate
        bl = forward - strafe + rotate
        br = forward + strafe - rotate
        max_val = max(abs(fl), abs(fr), abs(bl), abs(br), 1.0)

        fl = self.thrust * (fl / max_val)
        fr = self.thrust * (fr / max_val)
        bl = self.thrust * (bl / max_val)
        br = self.thrust * (br / max_val)

        self.motor_fl.throttle = self._safe(fl)
        self.motor_fr.throttle = self._safe(fr)
        self.motor_bl.throttle = self._safe(bl)
        self.motor_br.throttle = self._safe(br)

        if duration:
            time.sleep(duration)
            self.stop_all()

    def _safe(self, value):
        return max(-1.0, min(1.0, value))

    def stop_all(self):
        for m in [self.motor_fl, self.motor_fr, self.motor_bl, self.motor_br]:
            m.throttle = 0.0

    def keyboard_check(self):
        if self.kbhit():
            c = sys.stdin.read(1)
            if c == '\x1b':
                sys.stdin.read(1)
                arrow = sys.stdin.read(1)
                if arrow == 'A':
                    self.thrust = min(1.0, self.thrust + 0.1)
                    self.get_logger().info(f"Thrust increased: {self.thrust:.1f}")
                elif arrow == 'B':
                    self.thrust = max(0.5, self.thrust - 0.1)
                    self.get_logger().info(f"Thrust decreased: {self.thrust:.1f}")
            elif c == 's':
                self.active = True
                self.get_logger().info("Robot activated")
            elif c == ' ':
                self.active = False
                self.get_logger().info("Robot paused")

    def kbhit(self):
        dr, _, _ = select.select([sys.stdin], [], [], 0)
        return dr != []

def main(args=None):
    rclpy.init(args=args)
    settings = termios.tcgetattr(sys.stdin)
    tty.setcbreak(sys.stdin.fileno())

    node = ObstacleAvoidanceNode()
    try:
        rclpy.spin(node)
    finally:
        termios.tcsetattr(sys.stdin, termios.TCSADRAIN, settings)
        node.stop_all()
        node.destroy_node()
        rclpy.shutdown()
</code></pre>


    <h4>Step 3: Set Up <code>setup.py</code></h4>
    <pre><code>cd ~/ws_lidar/src/obstacle_avoidance
nano setup.py</code></pre>

    <p>Update the <code>entry_points</code> section:</p>
    <pre><code>entry_points={
    'console_scripts': [
        'obstacle_avoidance_controller = obstacle_avoidance.obstacle_avoidance_controller:main',
    ],
},</code></pre>

    <h4>Step 4: Build and Source Workspace</h4>
    <pre><code>cd ~/ws_lidar
colcon build
source install/setup.bash</code></pre>

    <h4>Step 5: Run Your Obstacle Avoidance Node</h4>
    <pre><code>ros2 run obstacle_avoidance obstacle_avoidance_controller</code></pre>

    <h4>Keyboard Controls</h4>
    <table>
      <thead>
        <tr><th>Key</th><th>Action</th></tr>
      </thead>
      <tbody>
        <tr><td>‚Üë</td><td>Increase thrust</td></tr>
        <tr><td>‚Üì</td><td>Decrease thrust</td></tr>
        <tr><td><code>s</code></td><td>Start movement</td></tr>
        <tr><td><code>space</code></td><td>Pause movement</td></tr>
        <tr><td><code>Ctrl+C</code></td><td>Stop all motors cleanly</td></tr>
      </tbody>
    </table>
  </details>
</section>




    <section class="card">
        <details>
          <summary>Download Project Files (Word Format)</summary>
          <p>You can download the full setup guides and step-by-step documentation as Word files using the links below:</p>
          <ul>
            <li>
              <a href="files/Lidar.docx" download>LiDAR Setup Guide (DOCX)</a>
            </li>
            <li>
              <a href="files/Logitech Cam.docx" download>USB Camera Setup Guide (DOCX)</a>
            </li>
            <li>
              <a href="files/Step by step guide(1).docx" download>Complete Step-by-Step Guide (DOCX)</a>
            </li>
          </ul>
          <p>You can print these out and scan the QR code (see below) to come back to the website any time!</p>
        </details>
      </section>
      <section class="card">
        <h2>üé¨ Video Demonstrations</h2>
        <p>Watch the LiDAR Car system in action, including manual control, autonomous mode, and full system movement.</p>
      
        <div class="video-container">
          <!-- Video 1: Autonomous Navigation -->
          <div class="video-block">
            <h3>Autonomous Navigation Mode</h3>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/TEothjIMlyU"
              title="Autonomous Navigation Mode" frameborder="0" allowfullscreen></iframe>
          </div>
      
          <!-- Video 2: Keyboard Manual Control -->
          <div class="video-block">
            <h3>Keyboard-Controlled Driving</h3>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/9ZjfeJcFqJo"
              title="Keyboard Control Driving" frameborder="0" allowfullscreen></iframe>
          </div>
      
          <!-- Video 3: LiDAR Car Movement Demo -->
          <div class="video-block">
            <h3>LiDAR Car Movement Demo</h3>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/f-S7wbVViuI"
              title="LiDAR Car Movement" frameborder="0" allowfullscreen></iframe>
          </div>
        </div>
      </section>      

      <section class="qr-section">
        <h3>Quick Access</h3>
        <p>Scan this QR code to return to the LiDAR Car Project website.</p>
        <img src="media/QRcode.jpg" alt="QR code to project website" class="qr-code" />
      </section>
      
  </main>

  <footer>
    <p>¬© 2025 Connor Reagan, Hugo Acevedo, Dhruveen Sheth</p>
  </footer>

  <script src="js/main.js"></script>
</body>
</html>
